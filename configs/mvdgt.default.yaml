# Default config to run MV-DGT end-to-end with `ptliq-run`
# Usage:
#   ptliq-run --config configs/mvdgt.default.yaml --workdir .
#
# Notes on sizing and recommendations (realistic prod scale):
# - If you train on ~100k trades/day for ~6 months (~120 trading days) → ~12M samples.
# - Recommended model size for 12M samples on a 24–48 GB GPU:
#     model.hidden: 128–192; model.heads: 2–4; model.dropout: 0.10–0.20
#     train.batch_size: 8k–16k (global, adjust for VRAM; use gradient accumulation if needed)
#     lr: 1e-3 with OneCycle or 3e-4 with cosine; epochs: 3–8 (early stop on val)
# - With CPU-only training, reduce hidden to 64–96 and batch_size to 512–2048.
# - Portfolio head (use_pf_head) improves portfolio sensitivity; keep it enabled for prod.
# - Correlation views can be heavy; if memory bound, consider removing 'corr_local' in view masks.

project:
  name: pt-liqadj
  seed: 42
  run_id: dgt_default

paths:
  raw_dir: data/raw/sim           # where bonds.parquet, trades.parquet will live
  interim_dir: data/interim       # validation reports etc.
  features_dir: data/features     # baseline features (unused here)
  models_dir: models              # baseline models root (unused here)
  reports_dir: reports            # baseline reports (unused here)

# Place the MV-DGT runner settings under data.mvdgt
# (keeps backward-compat with RootConfig)
data:
  mvdgt:

    # Locations (resolved relative to --workdir)
    graph_dir: data/graph          # where graph_nodes/edges live
    pyg_dir: data/pyg              # where PyG graph tensors live (pyg_graph.pt)
    outdir: data/mvdgt/dgt_default # where dataset artifacts (samples, masks, meta) go
    models_dir: models/mvdgt       # where trained MV-DGT model + metrics go

    # Dataset split ratios (test share = 1 - train - val)
    split_train: 0.70              # fraction of rows used for training
    split_val: 0.15                # fraction of rows used for validation

    # Optional: explicit path to trades parquet; default uses simulated or discovered location
    # trades_path: data/raw/sim/trades.parquet

    # Training and model hyperparameters (explicit; mirrors MVDGTTrainConfig/MVDGTModelConfig defaults)
    train:
      device: auto                 # "cuda"/"cpu" or "auto"; "auto" picks CUDA if available
      epochs: 5                    # total epochs; for 12M rows start with 3–8 and monitor val RMSE
      lr: 0.001                    # base LR; OneCycle in trainer ramps this; 3e-4 for cosine is also good
      weight_decay: 0.0001         # L2 regularization; 1e-4–3e-4 typical; increase if overfitting
      batch_size: 2048             # per-step batch (no grad accumulation here); set by VRAM: 8k–16k on 48GB
      seed: 17                     # RNG seed for reproducibility
      # Logging/monitoring
      enable_tb: true              # enable TensorBoard logging
      tb_log_dir: null             # when null → <outdir>/tb; set to a path to collate runs
      enable_tqdm: true            # progress bars during train/val loops
      log_every: 50                # batches between log lines (train loss, speed)
      # Attention logging (debug/analysis only; adds overhead)
      enable_attn_tb: false        # write attention stats to TB; off by default for speed
      attn_log_every: 200          # batch interval for attention logging
      # Portfolio similarity auxiliary loss (ID‑agnostic)
      portfolio_sim_weight: 0.0    # weight for within-portfolio variance loss on z_pf; try 0.02–0.10
      portfolio_jitter_std: 0.05   # std for optional portfolio size jitter in augments (future use)

    model:
      hidden: 192                  # main hidden size (node/heads). 128–192 for ~12M rows on GPU; 64–96 on CPU
      heads: 4                     # attention heads per TransformerConv; 2–4 typical; >4 rarely helps
      dropout: 0.10                # dropout in encoders/head; 0.1–0.2 recommended at scale
      trade_dim: 2                 # number of trade features in scorer (side_sign, log_size). Keep 2 unless you add features
      use_portfolio: true          # enable portfolio residual fusion over node embeddings (uses port_ctx)
      # Per‑sample portfolio head (concatenates portfolio vector at head; improves portfolio sensitivity)
      use_pf_head: true            # enable portfolio head; requires training with this flag to be effective
      pf_head_hidden: 128          # hidden width for pf head MLP; set to `hidden` by default; reduce if memory bound
      # Graph view names used by the model. Order must match masks in view_masks.pt
      views: [struct, port, corr_global, corr_local]  # drop 'corr_local' if memory constrained

      # Portfolio self-/cross-attention over request line items (per-portfolio only)
      # When enabled, the model will contextualize each line item with the rest of its portfolio via a compact Transformer.
      # This makes scores sensitive to portfolio composition (permutation-invariant within a portfolio; isolated across portfolios).
      # Defaults are conservative and suitable for 200–1000 line items per portfolio.
      use_portfolio_attn: true             # turn ON to enable portfolio self-/cross-attention; requires training with this flag
      portfolio_attn_layers: 1             # number of Transformer encoder layers over portfolio tokens; 1–2 recommended
      portfolio_attn_heads: 4              # attention heads inside portfolio encoder; 4 is a good default
      portfolio_attn_dropout: null         # dropout inside portfolio encoder; null→fallback to `dropout` above (typ. 0.10)
      portfolio_attn_hidden: null          # internal token dim for portfolio encoder; null→use `hidden`; reduce if VRAM is tight
      portfolio_attn_concat_trade: true    # include per-trade features (side_sign, log_size) in portfolio tokens for attention
      portfolio_attn_concat_market: false  # include market features in portfolio tokens (usually false; market also goes to head)
      portfolio_attn_mode: residual        # how to fuse portfolio context back: 'residual' adds a gated residual; 'concat' appends to head
      portfolio_attn_gate_init: 0.0        # logit init for the portfolio gate; 0.0→sigmoid(0)=0.5. Set negative (e.g., -2..-5) for safer warm-up
      max_portfolio_len: null              # optional cap per-portfolio length for attention; set 512/1024 for very large portfolios

      # Runtime‑computed/inferred fields (persisted for exact reconstruction at inference). Usually leave as null.
      x_dim: null                  # inferred from PyG node features at runtime; set only for debugging
      mkt_dim: null                # inferred from market context rows; set to 0 to disable market entirely
      use_market: null             # when null, inferred from mkt_dim>0; set true/false to force behavior
