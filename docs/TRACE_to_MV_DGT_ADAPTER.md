# TRACE → MV‑DGT adapter guide

This document shows how to adapt a real TRACE Enhanced trade feed plus reference data into the minimal parquet files expected by the MV‑DGT pipeline in this repo:

- trades.parquet — per‑trade rows with identifiers, minimal features, and a supervised label (residual in price bps)
- bonds.parquet — per‑bond node features used to build the graph and relation masks

It also provides a small, production‑oriented adapter snippet you can tailor to your environment.

Important: TRACE Enhanced does not include evaluated prices or a full security master. You will need a vendor (e.g., BVAL, ICE, Refinitiv) or an internal eval time series, and a security master with issuer/rating/sector/maturity, and a stable CUSIP↔ISIN crosswalk.


## 0) What the MV‑DGT stack actually consumes

MV‑DGT training/serving uses three inputs (see ptliq/features/build_mvdgt_dataset.py and DGTScorer):

1) trades.parquet — per‑trade features and label (residual in price bps)
2) bonds.parquet — bond node features; featurizers also derive relation masks from it + co‑trade/return stats
3) market_context.pt (+ market_index.parquet) — daily factor panel for date alignment (already generated by featurizers in this repo)

Only two numeric trade features are required at serve/train time:
- side_sign: +1 for customer buys, −1 for customer sells
- log_size: log of trade size (par) or DV01‑based proxy

The label is residual in price bps against an evaluated or reference fair price.


## 1) Minimal schemas required by MV‑DGT

### trades.parquet (minimal)
- isin: string (primary key to join into the graph)
- trade_dt: date or timestamp (used to map into market_index)
- side_sign: float (±1)
- log_size: float
- residual: float (supervised target, price bps) — either provided, or derived as 100*(exec_clean − eval_clean)
- Optional but recommended for portfolio context:
  - portfolio_id: string/integer grouping key; if not available, we will derive a heuristic from TRACE fields
  - exec_time: timestamp (execution time) — helps derive trade_dt and portfolio groups

Other columns may be present; the builder will ignore extras.

### bonds.parquet (minimal)
- isin: string (primary key)
- issuer_id or issuer: string/integer
- rating_num: numeric ordinal (AAA=1, AA+=2, …; consistent mapping)
- maturity_date: date (tenor is derived)
- sector_id: string/integer
- currency: string
- Optional but useful: amount_outstanding, vendor_liq / liq_score


## 2) Mapping from real data to the minimal schemas

### TRACE Enhanced → trades.parquet
- TRACE CUSIP (9‑char) → convert to ISIN (US+check digit) or join via a vendor crosswalk; store ISIN in output
- Execution Date/Time → exec_time (timestamp); derive trade_dt = date(exec_time)
- Buy/Sell Indicator (+ Reporting Party Type if needed) → side_sign ∈ {+1, −1}
- Quantity (par) → size → log_size = ln(max(size, 1))
- Trade Price (clean) → exec_clean (confirm price is clean vs dirty in your feed)
- As‑of/Late/Cancel/Corrected, ATS, Special Price → filters and optional heuristics for portfolio grouping
- Evaluated clean price (from vendor) → eval_clean → residual_bps = 100*(exec_clean − eval_clean)
- Portfolio grouping (if missing) → derive portfolio_id by clustering within tight time windows per dealer/day and using ATS/Special Price flags as hints

### Security master + Evals → bonds.parquet
- ISIN (or CUSIP mapped to ISIN)
- Issuer (ID or clean text mapped to a stable ID)
- Ratings (map to numeric ordinal rating_num)
- Sector (coarse sector ID)
- Maturity date
- Currency
- Amount outstanding (optional liquidity proxy)
- Liquidity score (if available)


## 3) Reference adapter: from TRACE CSVs to parquet

The snippet below illustrates a typical batch adapter using pandas. Replace column names to match your exact vendor dumps. This creates the two parquet files directly consumable by ptliq‑featurize and the MV‑DGT dataset builder.

```python
import pandas as pd
import numpy as np
from pathlib import Path

# --- Config
in_trace = Path("/path/to/trace_enhanced.csv")
sec_master = Path("/path/to/security_master.csv")
evals = Path("/path/to/evaluated_prices.csv")
# A crosswalk providing at least columns: cusip, isin
crosswalk = Path("/path/to/cusip_isin_crosswalk.csv")
out_dir = Path("data/raw/real")
out_dir.mkdir(parents=True, exist_ok=True)

# --- Helpers
RATING_ORDER = [
    "AAA","AA+","AA","AA-","A+","A","A-","BBB+","BBB","BBB-",
    "BB+","BB","BB-","B+","B","B-","CCC+","CCC","CCC-","CC","C","D","NR"
]
RATING_TO_NUM = {r:i+1 for i,r in enumerate(RATING_ORDER)}
RATING_TO_NUM.setdefault("NR", int(np.median(list(RATING_TO_NUM.values()))))

def to_rating_num(x):
    if pd.isna(x):
        return RATING_TO_NUM["NR"]
    x = str(x).strip().upper()
    return RATING_TO_NUM.get(x, RATING_TO_NUM["NR"])

# --- Load inputs
tr = pd.read_csv(in_trace)
sec = pd.read_csv(sec_master)
ev = pd.read_csv(evals)
xw = pd.read_csv(crosswalk)

# Normalize timestamps
tr["exec_time"] = pd.to_datetime(tr["execution_datetime"])  # your column name here
tr["trade_dt"] = tr["exec_time"].dt.normalize()

# Map CUSIP→ISIN
tr = tr.merge(xw[["cusip","isin"]], on="cusip", how="left")

# Side sign: assumes TRACE has customer buy/sell indicator in {"B","S"}
side_map = {"B": +1.0, "BUY": +1.0, "S": -1.0, "SELL": -1.0}
tr["side_sign"] = tr["buy_sell"].map(lambda s: side_map.get(str(s).upper(), 0.0))

# log_size from par quantity (fallback to 1 to avoid log(0))
tr["log_size"] = np.log(np.maximum(tr["quantity_par"].astype(float), 1.0))

# Clean executed price (TRACE price is usually clean dollar price)
tr["exec_clean"] = tr["trade_price"].astype(float)

# Merge evaluated price — align to same day (or intraday timestamp if available)
ev["asof_dt"] = pd.to_datetime(ev["asof_dt"]).dt.normalize()
tr = tr.merge(ev[["isin","asof_dt","eval_clean"]], left_on=["isin","trade_dt"], right_on=["isin","asof_dt"], how="left")

# Residual label in price bps
tr["residual"] = 100.0 * (tr["exec_clean"] - tr["eval_clean"])  # drop NA later if eval missing

# Portfolio grouping heuristics (optional)
# Example: per dealer per day within 2 min buckets, ATS/special flags joined to strengthen grouping
tr["dealer_day"] = tr["dealer_mpid"].astype(str) + "_" + tr["trade_dt"].dt.strftime("%Y%m%d")
tr["bucket"] = (tr["exec_time"].astype("int64") // (120 * 1_000_000_000))  # 2‑minute buckets
tr["portfolio_id"] = tr["dealer_day"].astype(str) + ":" + tr["bucket"].astype(str)

# Filter TRACE hygiene: drop cancels; keep final corrected; drop obvious bad rows
mask_ok = (~tr.get("cancel_flag", False).astype(bool))
tr = tr[mask_ok]

# Keep minimal columns for MV‑DGT
trades_out = tr[[
    "isin","trade_dt","side_sign","log_size","residual",
    "portfolio_id","exec_time"
]].copy()

# Drop rows missing critical fields
trades_out = trades_out.dropna(subset=["isin","trade_dt","side_sign","log_size","residual"])\
                       .reset_index(drop=True)

# --- Bonds table
# Expect columns in sec master: isin, issuer_id (or issuer), rating, sector_id, maturity_date, currency, amount_outstanding
b = sec.merge(xw[["cusip","isin"]], on="cusip", how="left") if "isin" not in sec.columns else sec.copy()

bonds_out = pd.DataFrame({
    "isin": b["isin"],
    "issuer_id": b.get("issuer_id", b.get("issuer")),
    "rating_num": b.get("rating_num", b.get("rating")).map(to_rating_num),
    "maturity_date": pd.to_datetime(b["maturity_date"]).dt.date,
    "sector_id": b["sector_id"],
    "currency": b["currency"],
    "amount_outstanding": b.get("amount_outstanding")
})

# Final hygiene
a = (~bonds_out["isin"].isna()) & (~bonds_out["maturity_date"].isna())
bonds_out = bonds_out[a].reset_index(drop=True)

# --- Write parquet
trades_out.to_parquet(out_dir/"trades.parquet", index=False)
bonds_out.to_parquet(out_dir/"bonds.parquet", index=False)
print("Wrote:", out_dir/"trades.parquet", out_dir/"bonds.parquet")

#residuals
import numpy as np
import pandas as pd

def compute_residual_bps(trades: pd.DataFrame,
                         exec_col: str = "price_clean_exec",
                         ref_mid_col: str = "price_clean_ref_mid",
                         side_col: str = "side") -> pd.DataFrame:
    """
    Returns a copy of `trades` with:
      - residual_bps : signed price-bps (cents per $100), positive = worse for both BUY and SELL
      - side_sign    : +1 for BUY/CBUY, -1 for SELL/CSELL (client view)
    """
    t = trades.copy()

    # 1) Side sign (client view): BUY/CBUY = +1, SELL/CSELL = -1
    side_map = {"BUY": 1.0, "CBUY": 1.0, "SELL": -1.0, "CSELL": -1.0}
    t["side_sign"] = (
        t[side_col].astype(str).str.upper().map(side_map).astype(float).fillna(0.0)
    )

    # 2) Clean prices (ensure numeric)
    pe = pd.to_numeric(t[exec_col], errors="coerce")
    pm = pd.to_numeric(t[ref_mid_col], errors="coerce")

    # 3) Target in *price-bps* (cents per $100), positive = worse for both sides
    t["residual_bps"] = t["side_sign"] * 100.0 * (pe - pm)

    # Optional: sanity guards
    t = t.dropna(subset=["residual_bps"]).copy()
    t["residual_bps"] = t["residual_bps"].clip(-1e4, 1e4)  # avoid wild outliers
    return t

```

Notes:
- Replace column names in the adapter to your feed’s exact names.
- If you lack evals, you can skip residual now and compute it later after joining a price reference; MV‑DGT training requires the residual label.
- If DV01 data is available, you can add dv01_per_100 and compute dv01_dollar = dv01_per_100 * exec_clean/100 * size_par/100; the MV‑DGT sampler can prefer DV01 weights when building portfolio context.


## 4) Building MV‑DGT artifacts from these parquet files

Once you have data/raw/real/trades.parquet and bonds.parquet, run the project’s featurization and builder steps. Example minimal sequence:

```bash
# 1) Validate (optional but recommended)
ptliq-validate --trades data/raw/real/trades.parquet --bonds data/raw/real/bonds.parquet

# 2) Featurize graph/pyg artifacts (ids, relation masks, market)
ptliq-featurize graph --trades data/raw/real/trades.parquet --bonds data/raw/real/bonds.parquet --outdir data/graph
ptliq-featurize pyg   --graph-dir data/graph --outdir data/pyg

# 3) Build MV‑DGT samples + view masks from artifacts
ptliq-mvdgt-build --trades data/raw/real/trades.parquet --graph-dir data/graph --pyg-dir data/pyg --outdir data/mvdgt/real_exp

# Output:
# - data/mvdgt/real_exp/samples.parquet   (per-trade rows indexed into node_id and market row_idx)
# - data/mvdgt/real_exp/view_masks.pt     (relation masks organized by view)
# - data/pyg/market_context.pt, market_index.parquet, portfolio_context.pt (if built)
```

Then you can train or score with the MV‑DGT components in this repo (see configs/mvdgt.default.yaml and ptliq/training/mvdgt_loop.py).


## 5) Practical hygiene and gotchas

- TRACE hygiene: drop cancels, treat corrections carefully (keep only final), map late/as‑of prints to the true execution date when aligning market context.
- Identifier stability: freeze a versioned CUSIP↔ISIN mapping per training snapshot for reproducibility.
- Rating normalization: keep a single ordinal scheme and apply it consistently.
- Portfolio grouping: if no explicit portfolio ID is present, the time‑bucket + dealer heuristic typically captures a large fraction of practical baskets; tune the bucket size and ATS/Special flags to your venue mix.
- Price conventions: ensure both executed and evaluated prices are clean (or both dirty), and in the same currency.


## 6) Minimal columns recap

Trades:
- isin, trade_dt (or exec_time), side_sign, log_size, residual, optional portfolio_id

Bonds:
- isin, issuer_id (or issuer), rating_num, maturity_date, sector_id, currency, optional amount_outstanding (and vendor_liq if available)

These are sufficient to run featurizers and MV‑DGT training/serving in this repository.